{"cells":[{"cell_type":"markdown","id":"e1d38d39","metadata":{"id":"e1d38d39"},"source":["**Universidade Estadual de Campinas - Unicamp**\n","\n","**Faculdade de Tecnologia - FT**\n","\n","**Autor:** Ulisses Martins Dias\n","\n","**Disciplina:** TT003 - Tópicos em Computação e Informática III\n","\n","**Atividade 05:** Treinando uma Rede Neural com Valores de Estado\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ulissesdias/tt003/blob/main/atividades/atividade05.ipynb)\n","\n","# Valores de Estado\n","\n","Em sala de aula, vimos o seguinte cenário.\n","\n","Temos um ambiente que fornece recompensas a uma agente que \"anda\" por um grid de maneira aleatória.\n","\n","\n","**Ambiente**: um grid $2 \\times 2$ consistindo de apenas quatro estados.\n","\n","**Estado**: temos apenas quatro estados, rotulados como A (NE), B (NO), C (SE) e D (SO).\n","\n","**Ação**: consiste em mover para cima, baixo, esquerda e direita. Ações que sairiam do grid simplesmente mantém o agente no lugar.\n","\n","1.  Digamos, por exemplo, que começamos no estado C, subir nos leva ao estado A.\n","2. Desse novo estado, se tentássemos nos mover para a esquerda, bateríamos em uma parede e permaneceríamos no estado A.\n","3. Seguir para a direita nos levaria para o estado B.\n","4. A partir daí, mover para baixo nos levaria ao estado D.\n","\n","**Recompensa**: vamos assumir que\n","1. a recompensa é 0 ao chegar em todos os estados, exceto quando o agente chegar ao estado B.\n","2. Se o agente chegar ao estado B, receberá uma recompensa de +5.\n","* Isso inclui começar no estado B e bater em uma parede para permanecer em B.\n","\n","**Política**: vamos considerar uma política que segue a distribuição uniforme de probabilidade. Em outras palavras, em qualquer estado, o agente tem 25% de probabilidade para se mover em qualquer direção.\n","\n","Essas configurações podem ser resumidas na figura a seguir:"]},{"cell_type":"markdown","id":"182b1b78","metadata":{"id":"182b1b78"},"source":["![alt text](http://www.ic.unicamp.br/~udias/si202/grid_world_01.png))"]},{"cell_type":"markdown","id":"4910729e","metadata":{"id":"4910729e"},"source":["Também vimos em sala de aula uma abordagem clássica e uma ambordagem frequentista para computar o valor esperado do ganho acumulado quando o fator de desconto era $\\gamma = 0.7$. O que percebemos é que, com o valor de desconto escolhido, o valor esperado do ganho acumulado se tornava o que mostramos a seguir:"]},{"cell_type":"markdown","id":"36530f33","metadata":{"id":"36530f33"},"source":["![tt003_aula02asdfasdf.gif](data:image/gif;base64,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)"]},{"cell_type":"markdown","id":"5cbe3357","metadata":{"id":"5cbe3357"},"source":["# Rede Neural\n","\n","Você deve estar se perguntando: 'Onde a rede neural entra na jogada?'.\n","\n","Bem, vamos usar a rede neural como nosso único mecanismo para guardar os valores de estado. Faremos isso ao tentar aprender esses valores de estado com uma abordagem clássica, ao fazer *bootstrapping*.\n","\n","Em outras palavras, em sala de aula guardamos os valores de estado em variáveis na memória principal e íamos atualizando periodicamente essas variáveis com novos valores até convergir. Entretanto, agora queremos que uma rede neural guarde esses valores. Assim:\n","\n","* A rede neural irá receber na entrada um estado. Você deve escolher por conta própria como representar um estado. Pode optar por um valor numérico (p. ex, 1,2,3,4), ou usar uma abordagem do tipo: $a = (1,0,0,0)$, $b = (0,1,0,0)$, e assim por diante.\n","\n","* A rede deve fornecer como saída uma predição do valor esperado do ganho acumulado para aquele estado passado na entrada.\n","\n","* A rede deve atualizar os valores dos pesos com uma função de custo apropriada.\n","\n","**Observação Importante:** no laço de treinamento da rede neural, use os quatro estados para atualizar os pesos da rede neural, como se os quatro estados fossem um mini-batch se o problema fosse de aprendizado supervisionado. Em outras palavras, dentro do laço de treinamento da rede neural:\n","\n","1. Use a própria rede neural para obter os valoress  esperados atuais de $a$, $b$, $c$ e $d$. Você fará isso pedindo à rede que forneça uma predição para esses estados.\n","2. Use o *bootstrapping* visto em sala de aula para computar os novos valores de $a$, $b$, $c$ e $d$.\n","3. Na função de custo, use os valores novos obtidos com o *bootstrapping* como *target*.\n","4. Execute o resto do treinamento como de costume.\n","\n","sempre atualize os valores dos quatro estados"]},{"cell_type":"code","source":["estados = [(1,0,0,0), (0,1,0,0), (0,0,1,0), (0,0,0,1)]\n","acoes   = [\"cima\", \"baixo\", \"esquerda\", \"direita\"]\n","novos_estados = {\n","  1: {'cima': 1, 'baixo': 3, 'esquerda': 1, 'direita': 2},\n","  2: {'cima': 2, 'baixo': 4, 'esquerda': 1, 'direita': 2},\n","  3: {'cima': 1, 'baixo': 3, 'esquerda': 3, 'direita': 4},\n","  4: {'cima': 2, 'baixo': 4, 'esquerda': 3, 'direita': 4}\n","}\n","policy  = [0.25, 0.25, 0.25, 0.25]\n","rewards = [0, 5, 0, 0]\n","gamma   = 0.7"],"metadata":{"id":"RpHFJe1jfUwH","executionInfo":{"status":"ok","timestamp":1698185046885,"user_tz":180,"elapsed":252,"user":{"displayName":"Marcelli Roberta Sarti","userId":"09433722157184530953"}}},"id":"RpHFJe1jfUwH","execution_count":222,"outputs":[]},{"cell_type":"code","source":["# Rede neural usando nn.Module\n","import torch.nn as nn\n","\n","class SimpleNeuralNetwork(nn.Module):\n","    def __init__(self):\n","        super(SimpleNeuralNetwork,self).__init__()\n","        self.input_layer    = nn.Linear(4,12)\n","        self.hidden_layer1  = nn.Linear(12,6)\n","        self.output_layer   = nn.Linear(6,1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self,x):\n","        out =  self.sigmoid(self.input_layer(x))\n","        out =  self.sigmoid(self.hidden_layer1(out))\n","        out =  self.sigmoid(self.output_layer(out))\n","        return out\n","\n","model = SimpleNeuralNetwork()"],"metadata":{"id":"7HCw2H5_8wAA","executionInfo":{"status":"ok","timestamp":1698185047177,"user_tz":180,"elapsed":5,"user":{"displayName":"Marcelli Roberta Sarti","userId":"09433722157184530953"}}},"id":"7HCw2H5_8wAA","execution_count":223,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","# pesos = torch.Tensor([0.0, 5.0, 0.0, 0.0])\n","loss_fn = nn.MSELoss()"],"metadata":{"id":"mSmecLyVCojx","executionInfo":{"status":"ok","timestamp":1698185047178,"user_tz":180,"elapsed":4,"user":{"displayName":"Marcelli Roberta Sarti","userId":"09433722157184530953"}}},"id":"mSmecLyVCojx","execution_count":224,"outputs":[]},{"cell_type":"code","source":["learning_rate = 0.1\n","optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n","\n","current_values = [0,0,0,0]\n","\n","for _ in range(20):  # Número de episódios\n","    for e, estado in enumerate(estados):\n","        next_value = []\n","        for a in acoes:\n","            predicted_value = model(torch.tensor(estado, dtype=torch.float32))\n","            novo_estado = novos_estados[e+1][a]\n","            novo_valor = model(torch.tensor(estados[novo_estado-1], dtype=torch.float32))\n","\n","            next_value.append(predicted_value)\n","\n","            target = policy[e] * (rewards[e] + gamma * novo_valor)\n","            loss = loss_fn(predicted_value, target)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        # Verificando a convergência\n","        diff_sum = sum(abs(current_values[i] - next_value[i]) for i in range(len(current_values)))\n","        if diff_sum <= 0.001:\n","            print(\"Convergiu\")\n","        current_values = next_value\n","\n","    print(loss.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RH2h1y7q9wh3","executionInfo":{"status":"ok","timestamp":1698185047738,"user_tz":180,"elapsed":563,"user":{"displayName":"Marcelli Roberta Sarti","userId":"09433722157184530953"}},"outputId":"2d264337-c865-4a69-b5e7-ad24d31069a3"},"id":"RH2h1y7q9wh3","execution_count":225,"outputs":[{"output_type":"stream","name":"stdout","text":["0.03200695291161537\n","0.09033288806676865\n","0.06380893290042877\n","0.03994692116975784\n","0.010500631295144558\n","0.004018130712211132\n","0.002506560878828168\n","0.0019519295310601592\n","0.0016594096086919308\n","0.0014676254941150546\n","0.0013271966017782688\n","0.0012184353545308113\n","0.001131423399783671\n","0.0010602825786918402\n","0.0010011681588366628\n","Convergiu\n","0.0009514184785075486\n","Convergiu\n","0.0009091111132875085\n","Convergiu\n","0.0008728231769055128\n","Convergiu\n","0.0008414702606387436\n","Convergiu\n","0.0008142185397446156\n"]}]},{"cell_type":"markdown","id":"1f0328b2","metadata":{"id":"1f0328b2"},"source":["# Forma de Avaliação\n","\n","Nesta atividade, você será avaliado por cumprir os seguintes itens.\n","\n","1. Consegue implementar uma rede neural.\n","2. Consegue escrever o código para treinamento da rede sem maiores imprecisões.\n","3. Consegue executar uma iteração do *bootstrapping*.\n","4. Consegue colocar o valor atual da rede e o valor do bootstrapping na função de custo.\n","5. Consegue analisar se a rede convergiu ou não para os valores de estado corretos.\n","\n","Note que na etapa 5 não foi exigido que você faça a rede encontrar os valores de estado corretos. Algumas decisões não muito boas podem fazer você não conseguir resolver o problema, mesmo com o código correto, dado que algum *feeling* que você ainda não tem pode ser necessário."]}],"metadata":{"kernelspec":{"display_name":"torch","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"colab":{"provenance":[{"file_id":"https://github.com/ulissesdias/tt003/blob/main/atividades/atividade05.ipynb","timestamp":1698164246081}]}},"nbformat":4,"nbformat_minor":5}