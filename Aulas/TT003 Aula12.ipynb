{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/ulissesdias/tt003/blob/main/notebooks/aula12_monte_carlo_com_jogo_da_velha_e_redes_neurais.ipynb","timestamp":1701383303924},{"file_id":"1AE1G-ydoFw80kMHQl6KHd7buqmzIRjax","timestamp":1701212169122},{"file_id":"1DLLV2CUl_WZRWqpSwEvRNLgu261J_pq5","timestamp":1700166337410}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"e031bea0898043ed930010d96b72f68f":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":["widget-interact"],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_bb1be023aa6c43699e50e67626f45871","IPY_MODEL_61e51b580f0e42f9aeaf8ce87c068866"],"layout":"IPY_MODEL_6ba4e6e248b74d86a19ffa34954a49f9"}},"bb1be023aa6c43699e50e67626f45871":{"model_module":"@jupyter-widgets/controls","model_name":"IntSliderModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntSliderModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"IntSliderView","continuous_update":true,"description":"frame","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_81eb6ff3d67c4c7fa9eb8a3aa7deaf45","max":5,"min":0,"orientation":"horizontal","readout":true,"readout_format":"d","step":1,"style":"IPY_MODEL_7369d50ed4364508af84bfce1d0f666f","value":2}},"61e51b580f0e42f9aeaf8ce87c068866":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","model_module_version":"1.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_e6c5151702da464a8f43e8b0d040b450","msg_id":"","outputs":[{"output_type":"display_data","data":{"text/plain":"<PIL.Image.Image image mode=RGB size=150x150>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAJYAAACWCAIAAACzY+a1AAAD2UlEQVR4nO3c207rQBBE0SLiw/Pp5wEEJrHH9lx796n9hgTB6qU4sWeSj+fzqWD9HFL1sRX+sP0xo03sc/UBdOviZLe/Fg2jrgyEjU8sOiSbsMv06ZBUwvLEK14LuZBIwjLDxb/d/eXn84lT5BG+j7hu6EeWOMXH6gO4Vy+/0Y85MxLhuKfL+0OBFDGEE0YMVWQQThsuURFAOHmsOEUA4bY5A43Pti06YYRpRjiGQtEJt80cZXC2baEJ164qUNY0QhO6K5kQH4Nw1Xks8vnzJwahK2RCfADCtWez+OdSAKErZ0J8JsRnQnwmxGdCfCbEZ0J8AEJf2pcDELpyJsTHIPRiUyEGoStkQnyhCb396UqhCV/yJsTdohNGGGWEYygUnfAlb8h/D0Doj8WUAxDKH04rxiCUPyJ6HIZQ/qD2QSRC+esS9kJ+acnRM+bW6I9+meUnIqGOT6FXLMtCOD9BCXX2fVsVEkS8r6iEX5Uhbz2Ic865/7aPgK8EoS7UQh3MboxLe++dKcQg1OpV+8iFJlx43xJ0yzQ0oRaNEuSn+ITyku9ZAEJ5ybcYg1Be8j0OQ6jBI4b6iUWoYYPm+glHqAHjRvuJSKiuQ6f7CUqoTqNP4CcuoZoBcvgJTagGhjR+ohOqCiOTnxIQ6iZJMj/lINRlmHx+SkOoCzwp/ZSJUEWkrH5KRqgDqsR+ykeoM7BkfkpJqEQfeblSTkIRNg/2yoT4chL6RMrOb2fY+aKCnS/t2fkGGzvf5mbnxSZ2XvJlE1ZjZFIEEzYypFGkEnYByKGIJOw4+gSKPMLuQ6crwggHjRutSCIcOmiuIoZwwoihigzCacMlKgIIJ48VpxidcMlAWYqhCReOEqQYmnDb/CFGZtvGIFw1TYqic865lj5efl549n//16Fein4OJtRRSXoEefccbS6gHgpwDWS/lr4vKnwRze33utC3sqD9ubT3DWVir3dnvKyDa+cGmxdXWe3fI/UWB1CHt7kHDdp+3SutVHQft/1GdLLY1HHo9hvU+Xphl9Hbb1yXlnwbAew3tKur9tUM9hvdjY0XFRj2m9C9vTO3SOw3p9vbny7C2G9aNTvYTnnsN7PKTYgFJPtNrn4f6S6V/ebXtBW4DGa/ObXu5r74dsaNq8OG/NO3M25oJsTnEyk+v53B54sKfL60x+cbbPh8mxufF5vweckXnzde4PP2J3zehIjPW4HxeUM+vkPCQeO2Yvf2CYcO2op92yGcMGIrduyVcNpwrdirP4STx2rFLv0SLhmoFdv7Jlw4Sis29lCAIZqtpcdyv7X/N0GltzOTs2Jd/wCWicVQfP8MVAAAAABJRU5ErkJggg==\n"},"metadata":{}}]}},"6ba4e6e248b74d86a19ffa34954a49f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81eb6ff3d67c4c7fa9eb8a3aa7deaf45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7369d50ed4364508af84bfce1d0f666f":{"model_module":"@jupyter-widgets/controls","model_name":"SliderStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"SliderStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"","handle_color":null}},"e6c5151702da464a8f43e8b0d040b450":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"cells":[{"cell_type":"markdown","source":["**Universidade Estadual de Campinas - Unicamp**\n","\n","**Faculdade de Tecnologia - FT**\n","\n","**Autor:** Ulisses Martins Dias\n","\n","**Disciplina:** TT003 - Tópicos em Computação e Informática III\n","\n","**Aula 11:** Monte Carlo com Jogo da Velha e Redes Neurais\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ulissesdias/tt003/blob/main/notebooks/aula12_monte_carlo_com_jogo_da_velha_e_redes_neurais.ipynb)"],"metadata":{"id":"NMicfPyYYRPa"}},{"cell_type":"markdown","metadata":{"id":"TLP3rFVYq_Ac"},"source":["# Monte Carlo com Jogo da Velha\n","\n","Em aulas anteriores, vimos o Monte Carlo sendo usado para criar um jogador mais inteligente para o jogo da velha. Entretanto, esse jogador colocava todos os *state-values* na memória, o que pode ser um problema no caso de muitos estados.\n","\n","Uma solução seria tentar usar uma rede neural para nos dizer os valores dos *state-values*. Se isso for possível, teremos uma rede-neural como oráculo, que será capaz de nos dizer qual a melhor jogada a realizar!\n","\n","Vamos começar redefinindo os *rewards* para treinar o nosso jogo."]},{"cell_type":"code","source":["## Efetuando o download do código da biblioteca\n","!wget https://raw.githubusercontent.com/ulissesdias/tt003/main/libraries/tic_tac_toe.py\n","\n","## Importando a biblioteca\n","import tic_tac_toe as ttt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SytrLekkLsud","executionInfo":{"status":"ok","timestamp":1701383355734,"user_tz":180,"elapsed":352,"user":{"displayName":"Marcelli Roberta Sarti","userId":"09433722157184530953"}},"outputId":"a610e553-335b-4d8f-d149-c58dfb128eae"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-11-30 22:29:20--  https://raw.githubusercontent.com/ulissesdias/tt003/main/libraries/tic_tac_toe.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3674 (3.6K) [text/plain]\n","Saving to: ‘tic_tac_toe.py’\n","\n","\rtic_tac_toe.py        0%[                    ]       0  --.-KB/s               \rtic_tac_toe.py      100%[===================>]   3.59K  --.-KB/s    in 0.001s  \n","\n","2023-11-30 22:29:20 (3.06 MB/s) - ‘tic_tac_toe.py’ saved [3674/3674]\n","\n"]}]},{"cell_type":"code","metadata":{"id":"tBPahaQ6MO0p"},"source":["def rewards(tab) :\n","    lwinner = ttt.winner(tab)\n","    if lwinner == 0 :\n","        return -1\n","    elif lwinner == 1 :\n","        return 10\n","    else :\n","       return -10"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Jogo inconcluso\n","rewards((-1,0,0,1,0,1,-1,0,0))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"puYF6aln8wX0","executionInfo":{"status":"ok","timestamp":1701278402999,"user_tz":180,"elapsed":9,"user":{"displayName":"Ulisses Martins Dias","userId":"04658399356803049089"}},"outputId":"5d8391d2-ab39-4a4c-d9cc-302b118d42e4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-1"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["## Vitória das brancas\n","rewards((1,0,-1, 1,-1,0,  1,0,0))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H0mmodyq88xq","executionInfo":{"status":"ok","timestamp":1701278402999,"user_tz":180,"elapsed":6,"user":{"displayName":"Ulisses Martins Dias","userId":"04658399356803049089"}},"outputId":"01c8c8c0-cf7d-40fc-8ecf-4e05831b12cc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"e4Eq-VCoKIuL"},"source":["## Rede Neural do Jogador com Política\n","\n","A inteligência do jogador depende exclusivamente da política e esta depende exclusivamente de quão bem calculados são os state-values. Como Monte Carlo pode ser usado para resolver o processo de **evaluation**, podemos melhorar a política com os novos **state-values**.  Isso foi o que aprendemos na aula passada.\n","\n","Agora temos que imaginar que os nossos state-values serão mantidos por uma rede neural. Para tanto, vamos começar analisando essa rede:"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn ## Pacote para adicionar camadas de Redes Neurais"],"metadata":{"id":"AVRVyK1uLxdC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TicTacToeNet(nn.Module):\n","    def __init__(self):\n","        super(TicTacToeNet, self).__init__()\n","        self.input_layer    = nn.Linear(9,128)\n","        self.hidden_layer1  = nn.Linear(128,64)\n","        self.output_layer   = nn.Linear(64,1)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        out =  self.relu(self.input_layer(x))\n","        out =  self.relu(self.hidden_layer1(out))\n","        out =  self.output_layer(out)\n","        return out\n"],"metadata":{"id":"cKKUlNJfL9fq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Instanciaremos agora a nossa rede neural. Essa rede ficará na memória como variável global para ser usada outras vezes."],"metadata":{"id":"nerOyKZ-9W4F"}},{"cell_type":"code","metadata":{"id":"ZjbsBn3UGPWc"},"source":["## Os state_values serão uma variável global neste notebook. Nele\n","## estará contido tudo aquilo que o agente precisa para fazer uma boa\n","## jogada\n","state_values_net = TicTacToeNet()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Vamos agora olhar as modificações no Aprendizado por Reforço."],"metadata":{"id":"WYObmz-FNA9Q"}},{"cell_type":"markdown","source":["Observe que os nossos *state_valeus* são uma instância da rede neural. Nesse caso, devemos mudar o nosso código para que atenda esse quesito. Observe que as mudanças não são tão grandes assim. Temos apenas que obter uma predição da rede quando quisermos os *state_values*."],"metadata":{"id":"ZKcRaSrzNTE9"}},{"cell_type":"code","source":["import random\n","\n","## Para computação dos state_values, usaremos gamma = 0.7 para\n","## descontos futuros\n","gamma = 0.7\n","\n","def player_rl(tab, turn, epsilon = 0) :\n","    ## A maioria do código é semelhante o da aula passada.\n","    valid_moves = ttt.get_valid_moves(tab)\n","\n","    epsilon_random = random.random()\n","    if epsilon_random < epsilon :\n","      return turn, random.choice(valid_moves)\n","    else :\n","      scores = []\n","      for move in valid_moves :\n","        next_state = list(tab)\n","        next_state[move] = turn\n","        ####################################################################\n","        ## A linha abaixo é a única coisa que mudou.\n","        ####################################################################\n","        scores.append(rewards(tuple(next_state)) + gamma*state_values_net(torch.tensor(next_state,  dtype =  torch.float)).item())\n","\n","      best_score = None\n","      if (turn == 1) :\n","          best_score = max(scores)\n","      else :\n","          best_score = min(scores)\n","\n","      best_moves = []\n","      for i in range(len(scores)) :\n","        if scores[i] == best_score :\n","          best_moves.append(valid_moves[i])\n","\n","      return turn, random.choice(best_moves)"],"metadata":{"id":"rvpZcTEyNk9e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y98zfCaxxCvl"},"source":["# Monte Carlo\n","\n","Na hora de atualizar os valores que correspondem à inteligência do jogador, precisamos entender que estamos atualizando os pesos de uma rede neural. Vamos configurar os hiperparâmetros da rede. Nada especial aqui, estamos configurando da mesma forma como configuramos para o banco de dados **iris** em aulas anteriores, exceto para a função de custo, dado que aqui temos um problema de regressão."]},{"cell_type":"code","source":["## Hiperparâmetros: note que tudo o que temos abaixo pode ser melhorado.\n","learning_rate = 0.01 # Taxa de aprendizado\n","loss_fn = torch.nn.MSELoss()\n","optimizer = torch.optim.Adam(state_values_net.parameters(),lr=learning_rate)\n","\n","batch_size = 16\n","epochs = 20 ## Não exagere aqui, nosso código não é otimizado"],"metadata":{"id":"PRUX7fFfPy2H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A seguir criamos uma função para realizar o treinamento. Nada muito diferente do que já aprendemos na disciplina."],"metadata":{"id":"hCd5o7VlRvKJ"}},{"cell_type":"code","source":["def train_network(x_train, y_train) :\n","    batch_no = len(x_train) // batch_size\n","\n","    for epoch in range(epochs):\n","\n","        ## Criando os MiniBatches\n","        for i in range(batch_no):\n","\n","            start = i * batch_size\n","            end = start + batch_size\n","\n","            x_var = x_train[start:end]\n","            y_var = y_train[start:end]\n","\n","            # Limpando os gradientes do último uso de loss.backward()\n","            optimizer.zero_grad()\n","\n","            # Passo \"forward\"\n","            output_train = state_values_net(x_var)\n","\n","            # Computando a loss\n","            loss = loss_fn(output_train, y_var)\n","\n","            # backward: calculando os gradientes\n","            loss.backward()\n","\n","            # Optimizando os pesos com o gradiente computado\n","            optimizer.step()"],"metadata":{"id":"51HCdlC1RzvL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Vejamos alguns resultados antes do treino"],"metadata":{"id":"zp2NNFTLonmb"}},{"cell_type":"code","source":["## Vitória das brancas no próximo lance\n","state_values_net(torch.tensor([-1,0,0,1,0,1,-1,0,0], dtype = torch.float))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y8mpLRbeok-4","executionInfo":{"status":"ok","timestamp":1701278412530,"user_tz":180,"elapsed":27,"user":{"displayName":"Ulisses Martins Dias","userId":"04658399356803049089"}},"outputId":"ffa907ab-f3c5-4483-9ae8-dc83c081f39e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.1234], grad_fn=<ViewBackward0>)"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["## Vitória das pretas no próximo lance\n","state_values_net(torch.tensor([-1,-1,0,1,0,1,1,0,0], dtype = torch.float))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MbgavyFSo0Ou","executionInfo":{"status":"ok","timestamp":1701278412530,"user_tz":180,"elapsed":24,"user":{"displayName":"Ulisses Martins Dias","userId":"04658399356803049089"}},"outputId":"4e8848b2-c7cc-4a96-bc3b-e2ea528df557"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.1308], grad_fn=<ViewBackward0>)"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"Cwof1vmfKT1O"},"source":["Finalmente, vamos começar a atualizar a rede neural. Faremos primeiramente uma inicialização com jogos ao acaso para seguir a metodologia padrão. O modo como inicializaremos é simples:\n","\n","1. Geraremos um número de episódios.\n","2. Computaremos a soma dos retornos descontados para cada estado alcançado nestes episódios e salvaremos em memória.\n","3. Transformaremos esses retornos em tensores.\n","4. Treinaremos a rede.\n","\n","O passo 2 é igual à função **update_state_values_monte_carlo** que tínhamos na aula anterior. Entretanto, ao invés de atualizar os **state_values**, salvaremos em memória algumas poucas instâncias para treinar a rede. Veja a função a seguir:"]},{"cell_type":"code","source":["## Código de aulas anteriores\n","def update_state_values_monte_carlo(rewards, state_values_dict, amostra, gamma = 0.7, alpha = 0.1) :\n","    discounted_return = 0\n","    next_state        = amostra[-1]\n","\n","    for state in reversed(amostra[:-1]) :\n","        discounted_return = gamma*discounted_return + rewards(next_state)\n","\n","        state_values_dict[state] = state_values_dict.get(state,0) + alpha*(discounted_return - state_values_dict.get(state, 0))\n","        next_state = state\n"],"metadata":{"id":"u2YH8zXYT7aJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Vamos exemplificar os quatros passos a seguir. Como a rede ainda não tem conhecimento de nada, vamos colocar epsilon = 1 no jogador."],"metadata":{"id":"pHvUgZdmTpno"}},{"cell_type":"code","source":["# 1. Geraremos um número de episódios.\n","# 2. Computaremos a soma dos retornos descontados para cada estado alcançado nestes episódios.\n","state_values_dict = {}\n","for i in range(100) :\n","    _, amostra = ttt.game(player_rl, player_rl, epsilon = 1)\n","    update_state_values_monte_carlo(rewards, state_values_dict, amostra, gamma = 0.7, alpha = 0.1)\n","\n","\n","# 3. Transformaremos esses retornos em tensores.\n","x_train = torch.tensor(list(state_values_dict.keys()),   dtype = torch.float)\n","y_train = torch.tensor(list(state_values_dict.values()), dtype = torch.float)\n","y_train = torch.unsqueeze(y_train, -1)\n","\n","# 4. Treinaremos a rede.\n","train_network(x_train, y_train)"],"metadata":{"id":"lvOwkQyETUwR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Vejamos alguns resultados da rede após essa  inicialização."],"metadata":{"id":"q1XgZRzmXNv7"}},{"cell_type":"code","metadata":{"id":"clwd_GseKTXG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701278413354,"user_tz":180,"elapsed":10,"user":{"displayName":"Ulisses Martins Dias","userId":"04658399356803049089"}},"outputId":"b54007d6-58d2-404d-f850-48ee1b99b8f5"},"source":["## Vitória das brancas no próximo lance\n","state_values_net(torch.tensor([-1,0,0,1,0,1,-1,0,0], dtype = torch.float))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-0.6430], grad_fn=<ViewBackward0>)"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["## Vitória das pretas no próximo lance\n","state_values_net(torch.tensor([-1,-1,0,1,0,1,1,0,0], dtype = torch.float))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cO_PzgqvXSp5","executionInfo":{"status":"ok","timestamp":1701278413354,"user_tz":180,"elapsed":6,"user":{"displayName":"Ulisses Martins Dias","userId":"04658399356803049089"}},"outputId":"dee8b8ab-7b32-49c4-d5ad-c155e3138ca9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-0.7857], grad_fn=<ViewBackward0>)"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["Verifique agora que os tabuleiros possuem **state_values** mais condizentes com a realidade."],"metadata":{"id":"uq2sGzQRBBC2"}},{"cell_type":"markdown","metadata":{"id":"EL3EKCv6OPIc"},"source":["## Efetuando Aprendizado\n","\n","Agora que temos uma rede inicializada, podemos usar ela mesma para obter os valores de estado atuais antes de gerar os valores de estado futuros. O que precisamos fazer agora é o seguinte:\n","\n","1. Colocar o agente que usa como inteligência a rede neural para jogar contra si mesmo algumas vezes. Isso gerará episódios que nos darão novos retornos descontados acumulados.\n","2. Os novos retornos descontados acumulados devem ser salvos na memória. Podemos usar um dicionário para isso.\n","3. Transformamos esses retornos em tensores.\n","4. Treinamos a rede com eles.\n","\n","Devemos lembrar que a rede neural agora contém os valores de estado. Nesse caso, a função  **update_state_values_monte_carlo** deve dar espaço para uma nova implementação em que é feita uma busca pelo valor de estado na rede."]},{"cell_type":"code","source":["## Código de aulas anteriores\n","def update_state_values_monte_carlo_neural_network(rewards,\n","                                                   state_values_dict,\n","                                                   state_values_net,\n","                                                   amostra,\n","                                                   gamma = 0.7,\n","                                                   alpha = 0.1) :\n","    discounted_return = 0\n","    next_state        = amostra[-1]\n","\n","    for state in reversed(amostra[:-1]) :\n","        discounted_return = gamma*discounted_return + rewards(next_state)\n","\n","        ## Obtendo os valores de estado. Se não estiver no dicionário,\n","        ## então procurar na rede neural.\n","        previous_state_value = None\n","        if (state in state_values_dict) :\n","            previous_state_value = state_values_dict[state]\n","        else :\n","            previous_state_value = state_values_net(torch.tensor(state,  dtype = torch.float))\n","\n","        state_values_dict[state] = previous_state_value + alpha*(discounted_return - previous_state_value)\n","        next_state = state\n"],"metadata":{"id":"5plw9DHL2amD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Agora vamos efetuar os quatro passos acima mencionados em um laço. Sempre fazendo o jogador jogar contra si mesmo."],"metadata":{"id":"pScQMLM36-I8"}},{"cell_type":"code","metadata":{"id":"Sbb4S908OT91","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701278523555,"user_tz":180,"elapsed":109696,"user":{"displayName":"Ulisses Martins Dias","userId":"04658399356803049089"}},"outputId":"b361358a-c2c6-4240-a1f3-af6b6099c08e"},"source":["ITERACOES = 100 ## Não exagere aqui, nosso código não é otimizado\n","for i in range(ITERACOES) :\n","    epsilon = max(0.1, (ITERACOES - i)/ITERACOES)\n","\n","    if (i % (ITERACOES/10) == 0) :\n","        print(\"ITERAÇÃO %i\" % i)\n","\n","    # 1. Geraremos um número de episódios.\n","    # 2. Computaremos a soma dos retornos descontados para cada estado alcançado nestes episódios.\n","    state_values_dict = {}\n","    for i in range(100) :\n","        _, amostra = ttt.game(player_rl, player_rl, epsilon = 1)\n","        update_state_values_monte_carlo_neural_network(rewards, state_values_dict,\n","                                                       state_values_net,\n","                                                       amostra, gamma = 0.7, alpha = 0.1)\n","\n","    # 3. Transformaremos esses retornos em tensores.\n","    x_train = torch.tensor(list(state_values_dict.keys()),   dtype = torch.float)\n","    y_train = torch.tensor(list(state_values_dict.values()), dtype = torch.float)\n","    y_train = torch.unsqueeze(y_train, -1)\n","\n","\n","    # 4. Treinaremos a rede.\n","    train_network(x_train, y_train)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ITERAÇÃO 1\n","ITERAÇÃO 2\n","ITERAÇÃO 3\n","ITERAÇÃO 4\n","ITERAÇÃO 5\n","ITERAÇÃO 6\n","ITERAÇÃO 7\n","ITERAÇÃO 8\n","ITERAÇÃO 9\n","ITERAÇÃO 11\n","ITERAÇÃO 12\n","ITERAÇÃO 13\n","ITERAÇÃO 14\n","ITERAÇÃO 15\n","ITERAÇÃO 16\n","ITERAÇÃO 17\n","ITERAÇÃO 18\n","ITERAÇÃO 19\n","ITERAÇÃO 21\n","ITERAÇÃO 22\n","ITERAÇÃO 23\n","ITERAÇÃO 24\n","ITERAÇÃO 25\n","ITERAÇÃO 26\n","ITERAÇÃO 27\n","ITERAÇÃO 28\n","ITERAÇÃO 29\n","ITERAÇÃO 31\n","ITERAÇÃO 32\n","ITERAÇÃO 33\n","ITERAÇÃO 34\n","ITERAÇÃO 35\n","ITERAÇÃO 36\n","ITERAÇÃO 37\n","ITERAÇÃO 38\n","ITERAÇÃO 39\n","ITERAÇÃO 41\n","ITERAÇÃO 42\n","ITERAÇÃO 43\n","ITERAÇÃO 44\n","ITERAÇÃO 45\n","ITERAÇÃO 46\n","ITERAÇÃO 47\n","ITERAÇÃO 48\n","ITERAÇÃO 49\n","ITERAÇÃO 51\n","ITERAÇÃO 52\n","ITERAÇÃO 53\n","ITERAÇÃO 54\n","ITERAÇÃO 55\n","ITERAÇÃO 56\n","ITERAÇÃO 57\n","ITERAÇÃO 58\n","ITERAÇÃO 59\n","ITERAÇÃO 61\n","ITERAÇÃO 62\n","ITERAÇÃO 63\n","ITERAÇÃO 64\n","ITERAÇÃO 65\n","ITERAÇÃO 66\n","ITERAÇÃO 67\n","ITERAÇÃO 68\n","ITERAÇÃO 69\n","ITERAÇÃO 71\n","ITERAÇÃO 72\n","ITERAÇÃO 73\n","ITERAÇÃO 74\n","ITERAÇÃO 75\n","ITERAÇÃO 76\n","ITERAÇÃO 77\n","ITERAÇÃO 78\n","ITERAÇÃO 79\n","ITERAÇÃO 81\n","ITERAÇÃO 82\n","ITERAÇÃO 83\n","ITERAÇÃO 84\n","ITERAÇÃO 85\n","ITERAÇÃO 86\n","ITERAÇÃO 87\n","ITERAÇÃO 88\n","ITERAÇÃO 89\n","ITERAÇÃO 91\n","ITERAÇÃO 92\n","ITERAÇÃO 93\n","ITERAÇÃO 94\n","ITERAÇÃO 95\n","ITERAÇÃO 96\n","ITERAÇÃO 97\n","ITERAÇÃO 98\n","ITERAÇÃO 99\n"]}]},{"cell_type":"markdown","source":["Verifique agora que os tabuleiros possuem **state_values** mais condizentes com a realidade."],"metadata":{"id":"GaEK8M-FB-bb"}},{"cell_type":"code","source":["## Vitória das brancas no próximo lance\n","state_values_net(torch.tensor([-1,0,0,1,0,1,-1,0,0], dtype = torch.float))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q_H_K49BlRym","executionInfo":{"status":"ok","timestamp":1701278523555,"user_tz":180,"elapsed":10,"user":{"displayName":"Ulisses Martins Dias","userId":"04658399356803049089"}},"outputId":"a2154e3b-bf5c-4f60-c523-15d736a802df"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.1875], grad_fn=<ViewBackward0>)"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["## Vitória das pretas no próximo lance\n","state_values_net(torch.tensor([-1,-1,0,1,0,1,1,0,0], dtype = torch.float))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wF-otgzIpafI","executionInfo":{"status":"ok","timestamp":1701278523555,"user_tz":180,"elapsed":6,"user":{"displayName":"Ulisses Martins Dias","userId":"04658399356803049089"}},"outputId":"759b7c8e-8b33-4a30-b331-4eaca1f6a258"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-3.9969], grad_fn=<ViewBackward0>)"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"aoSs-g4nP57C"},"source":["## Validando Resultados\n","\n","Vamos agora colocar o possante jogador contra alguém ao acaso."]},{"cell_type":"code","source":["def campeonato(player1, player2, num_games = 1000) :\n","    results = [0,0,0]\n","    for i in range(num_games) :\n","        result, x = ttt.game(player1, player2)\n","        results[result] += 1\n","    return tuple(results)"],"metadata":{"id":"nokft8BFS2Dc"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8gI5bUdHXQld","executionInfo":{"status":"ok","timestamp":1701278530882,"user_tz":180,"elapsed":7330,"user":{"displayName":"Ulisses Martins Dias","userId":"04658399356803049089"}},"outputId":"f0e26a8c-1104-4c25-d1a0-e1ae0c795fca"},"source":["results = campeonato(ttt.random_player, player_rl)\n","print(\"Draw = %i, Acaso = %i, RL = %i\" % results)\n","\n","results = campeonato(player_rl, ttt.random_player)\n","print(\"Draw = %i, RL = %i, Acaso = %i\" % results)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Draw = 97, Acaso = 52, RL = 851\n","Draw = 21, RL = 949, Acaso = 30\n"]}]},{"cell_type":"markdown","source":["O jogador ficou melhor do que era antes? A distribuição de vitórias e derrotas é melhor do que o jogador ao acaso?"],"metadata":{"id":"CDtM8Z8Y3yA_"}},{"cell_type":"markdown","source":["# Desafiando Humanos"],"metadata":{"id":"LFi5eEGO37AK"}},{"cell_type":"markdown","metadata":{"id":"0mxeIBzKVOW8"},"source":["Que tal agora desafiar um humano? Vamos gerar um código que torne isso possível, não faremos nada muito mirabolante, apenas trocaremos as chamadas de **print** para **display(board)**.\n"]},{"cell_type":"code","source":["def human_player(tab, turn) :\n","    valids  = ttt.get_valid_moves(tab)\n","    print(valids)\n","\n","    board = ttt.draw_board( tab )\n","    display(board)\n","    move = int(input())\n","\n","    while not move in valids :\n","        move = int(input())\n","    return turn, move"],"metadata":{"id":"u_exnz7y4BHu"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oqn8XI7cP44i","colab":{"base_uri":"https://localhost:8080/","height":721},"executionInfo":{"status":"ok","timestamp":1701278567392,"user_tz":180,"elapsed":36525,"user":{"displayName":"Ulisses Martins Dias","userId":"04658399356803049089"}},"outputId":"b08e06e5-d297-4f9d-ce77-31ff7f67653d"},"source":["result, history = ttt.game(player_rl, human_player)\n","ttt.draw_board(history[-1])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0, 1, 2, 3, 5, 6, 7, 8]\n"]},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=150x150>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAJYAAACWCAIAAACzY+a1AAACWUlEQVR4nO3cUY6CQBQF0eusnKX7NzEGHZSm+9Wbqg3YuScxgIbbtm0p1u+RSp2t5qmS/Kw+gJ1NQnwS4pMQn4T4JMQnIT4J8UmIT0J8EuKTEJ+E+CTEJyE+CfFJiE9CfBLikxCfhPgkxCchPgnxSYhPQnwS4pMQn4T4JMQnIT4J8UmIT0J8EuKTEJ+E+CTEJyE+CfFJiE9CfBLikxCfhPgkxCchPgnNzOy/d6v2punsvfx64SFLHWY3xuXMqtWqae3GIMyKNRF+KU74NOLMTRd+9KeVJsyiKUF+qU+Y6YOy/IIgzMRZcX6hEGbKuES/gAhz8cRQv7AIc9nQXL/gCHPB3Gi/EAkzdHS6X6CEGTR9A79wCXMaoIdf0IQ5wdDGL3TCfIXRyS8NCPMhSTO/9CDMYZh+fmlDmAM8Lf3SiTBvkbr6pRlhXlA19ks/wvwF1swvLQlz+HKmRz0JQ/jz4KgkxNeT0C9Sdl7OsPOmgp239ux8wMbOx9zs/LGJnT/5sgm/xuikCCY8ydBGkUo4BKCHIpJw4PQNFHmEw0enK8IIL5obrUgivHRoriKGcMLEUEUG4bRxiYoAwsmz4hSrEy4ZlKVYmnDhlCDF0oSPzR+xMttjDMJVa1IUzczsTKXfClzqbDVPFcrljL1JQnwS4pMQn4T4JMQnIT4J8UmIT0J8EuKTEJ+E+CTEJyE+CfFJiE9CfBLikxCfhPgkxCchPgnxSYhPQnwS4pMQn4T4JMQnIT4J8UmIT0J8EuKTEJ+E+CTEJyE+CfFJiE9CfBLikxCfhPgkxCchvjvp/RCtU3qqvgAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["1\n","[0, 2, 3, 5, 7, 8]\n"]},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=150x150>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAJYAAACWCAIAAACzY+a1AAAD2UlEQVR4nO3c207rQBBE0SLiw/Pp5wEEJrHH9lx796n9hgTB6qU4sWeSj+fzqWD9HFL1sRX+sP0xo03sc/UBdOviZLe/Fg2jrgyEjU8sOiSbsMv06ZBUwvLEK14LuZBIwjLDxb/d/eXn84lT5BG+j7hu6EeWOMXH6gO4Vy+/0Y85MxLhuKfL+0OBFDGEE0YMVWQQThsuURFAOHmsOEUA4bY5A43Pti06YYRpRjiGQtEJt80cZXC2baEJ164qUNY0QhO6K5kQH4Nw1Xks8vnzJwahK2RCfADCtWez+OdSAKErZ0J8JsRnQnwmxGdCfCbEZ0J8AEJf2pcDELpyJsTHIPRiUyEGoStkQnyhCb396UqhCV/yJsTdohNGGGWEYygUnfAlb8h/D0Doj8WUAxDKH04rxiCUPyJ6HIZQ/qD2QSRC+esS9kJ+acnRM+bW6I9+meUnIqGOT6FXLMtCOD9BCXX2fVsVEkS8r6iEX5Uhbz2Ic865/7aPgK8EoS7UQh3MboxLe++dKcQg1OpV+8iFJlx43xJ0yzQ0oRaNEuSn+ITyku9ZAEJ5ybcYg1Be8j0OQ6jBI4b6iUWoYYPm+glHqAHjRvuJSKiuQ6f7CUqoTqNP4CcuoZoBcvgJTagGhjR+ohOqCiOTnxIQ6iZJMj/lINRlmHx+SkOoCzwp/ZSJUEWkrH5KRqgDqsR+ykeoM7BkfkpJqEQfeblSTkIRNg/2yoT4chL6RMrOb2fY+aKCnS/t2fkGGzvf5mbnxSZ2XvJlE1ZjZFIEEzYypFGkEnYByKGIJOw4+gSKPMLuQ6crwggHjRutSCIcOmiuIoZwwoihigzCacMlKgIIJ48VpxidcMlAWYqhCReOEqQYmnDb/CFGZtvGIFw1TYqic865lj5efl549n//16Fein4OJtRRSXoEefccbS6gHgpwDWS/lr4vKnwRze33utC3sqD9ubT3DWVir3dnvKyDa+cGmxdXWe3fI/UWB1CHt7kHDdp+3SutVHQft/1GdLLY1HHo9hvU+Xphl9Hbb1yXlnwbAew3tKur9tUM9hvdjY0XFRj2m9C9vTO3SOw3p9vbny7C2G9aNTvYTnnsN7PKTYgFJPtNrn4f6S6V/ebXtBW4DGa/ObXu5r74dsaNq8OG/NO3M25oJsTnEyk+v53B54sKfL60x+cbbPh8mxufF5vweckXnzde4PP2J3zehIjPW4HxeUM+vkPCQeO2Yvf2CYcO2op92yGcMGIrduyVcNpwrdirP4STx2rFLv0SLhmoFdv7Jlw4Sis29lCAIZqtpcdyv7X/N0GltzOTs2Jd/wCWicVQfP8MVAAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["2\n","[0, 3, 5, 8]\n"]},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=150x150>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAJYAAACWCAIAAACzY+a1AAAEtUlEQVR4nO2c2XbbMAxExz79cH16H7rEsSWKCwjMILhvbWRyiBvZXOQ8juMAGf8jTWdrvHC9TapUAH5Nv5KNziq8Xubw6+uQKoPCxdtik0i3VNoKTapvLtI5larC9tgmPnVMRIakklTYHnDna08vPo7DfLayO5Wews/BzBX9qmpzFgNTPSe6CcSqUrZtxqZSUmhyu1y1/Nm4SioZhdMl3toFQyoNhQ6VmuiIJJWAQrdKDXXHk0pA4Su7KzXXS2wqdoU+1RnNQJWKXeErnoWbnpFu5bSvB8Mv1BunkUJy3nYanuo4DqW7sDilFGojcxdGvdsPrQvd0FsXFm1KoTwCCmPnzJ27M84I784Un5RCeUqhPKVQnlIoTymUpxTKUwrlEVDIs4ju+X8fammfilIoj4ZCkmOdoZ/uow6bUiFz5Fs0oFb49pwPSe9sqagVvhH+uN/ileuc9sWukOEZyXoU2JJ6IP8TAYU8X0CZuMyKRncCCkHzNbCVi1dod6ShEBxfxjR5ySi3XcgoxNlgrEr22dT0jNQ/lZJC7JkcrrcZm0ryj5Zc/W4OFa5znsKfSk8hrt+seqo2NEORSCWpEP9GZXgnmXyAhaRSVfiHdsmGGjGEM1VRFEVxhcafSwgMSRXmFI2lfVTV2GydoqEQ0efjzFAr7NkhzNf1KNQKEVRKIX/gVwimw1VOBBSC5nCVEw2F4Dhc5URGITaXWNQftBRiW6F1/UFOITaUW9ofFBXCtOjq/iCqEEalT+APugqxLCCHP0grxIKGNP6grhBTMjL5QwKFGFSSzB9yKES3mHz+kEYhOvSk9IdMCtGUlNUfkinEharE/pBPIe6EJfOHlAph+kg8PzkVQuHhQStKoTw5FdYbqTY1ndGmFhXa1NJem9pg06a2ubWpwyZt6shXW+G0jEwWhRUuakhjUVWhiYAcFiUVGpY+gUU9heZFV7copnBTuaUtKincWmhdizIKHUosalFDoVtxFS0KKHQuq5xFdoUhBdWySK0wsJRCFqkVvuJfRGZtr2gojKqmisWiKIpihcfbvwPf/RtdU6Vi+4B8ksye2/1ypiLhCYI1UE+PnKkY+Luo4FxEc6Zi42tdyLmVxZmKim9Le84NZc5UPLzvznAe63CmIuFkg81hGBNdcKZi4HyPdOtgphvnTBXO5Tb3piEtNsuZKpbWSYX5wEwa5EwVyM1hk+Hw0jcVxf15YeJbJ4E/dB75pvwAy+EP/af2yaaRafxh6MGLNIu5TP4w+uzM0ODdKsWZyo0xhRDf2MznD3NPsN0WIqRSnKkcmFGIZjkCK8WZajeTCnFRlPBKcabayrxC3JUmqlKcqfaxpBDdEwdnOFNtYlUhOiYOIXCm2kEplKfeSOWp6Yw8taiQp5b28tQGmzy1zS1PHTa1LpZgQOHE4B3qxZnKk16F08PeWi/OVM50KVwc8KZ6caby516hyVDN68WZKoQbhYaDTN9UFC2FP+TWUbd4qfBHfYBJWzxX+AOnkboWTxT+2MWcqMV3hW7DGOqIMxUJ3xQ6D6CzO85UPHwpDIl+2ylnKioef/LFhu7sjiQVm9Enov119siZioFnuL+efjlTkdCazjjD+cbFb/E391Zz7ENHuV4AAAAASUVORK5CYII=\n"},"metadata":{}},{"name":"stdout","output_type":"stream","text":["0\n"]},{"output_type":"execute_result","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=150x150>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAJYAAACWCAIAAACzY+a1AAAEvElEQVR4nO2c2ZbaMBBEC598OJ+eh8yZIYysfalq6r4RGHWpLxjJdng8n08MkPnz7pG//3B8hKanKsekSgXgz9wody8bfKO0lqt8WYxUbQoH34CLWvbhqWoVTpnn9JY5FWoU5kfpOL5PaZlTffPoq9o01dZBigsHp3olp/D3U9PfpMkB881yqjeu8SHqGR/TqX6T/hS+/eN4oKbx797vTpUcP/EpXJ2pr4RT3ZV4V7ghU0chp8o8vOpfOp3Kck6VL9ewnFnB+MJhBVqpruIrdrJiaTcOear0p3BnxPpaTpWs9bWpeH3uyFusWNSpkkWfz+ftd6FR4QLHkd30kfgUntLZtAPbhkQqH0jlsUJ5Hq8Pjn8pJgM4VZLCvtAIYYXyWKE8ViiPFcpjhfJYoTxWKM91/DJTsbpT5av7UyiPFcrji005JFL5kq82vvEiAl8Kz65L76o71W9+V/dNiKO1jqfyrcBlyFP5hvwJVc6m8n+L6SzHk6qwL1yXrKmQU2UeJg6kG5J1lHCquxK3P5ew6Du8Ztjvf2l68Wemgn8uYcqYZ1P5d2e0U6GosJgg8+z4Hy4dPEYq1CisGaWJ+jfN4NurCdFUaP0ZvcFwE+f2OqBTGWOMGaF2ObOTFdusbqjCJLnd2lNxqmtstpJoKMSJbkr4A7nCtybu7OnB0q1QK8ShVgr5A79CbG+olj9IKATNxVVONBSC4+IqJzIKsbjFov6gpRDLGq3rD3IKsaDd0v6gqBBTm67uD6IKMan1AfxBVyGGBcTwB2mFGNAQxh/UFaJLRiR/CKAQjUqC+UMMhagWE88fwihEhZ6Q/hBJIbKSovpDMIW4URXYH+IpRElYMH8IqRDVy5kYxFQIhZsHZ2GF8sRU6AOpNl7OaONNhTbe2mvjE2za+DS3Nr7YpI0v+Wor7JYRyaKwwkENYSyqKpwiIIZFSYUTWx/Aop7C6U1XtyimcFG7pS0qKVzaaF2LMgo3tFjUoobCbc1VtCigcHNb5SyyKzzSUC2L1AoPtlLIIrXCV/Y3kVnbKxoKT3VTxaIxxpgRHm+PDx79M6WpUrF9QV4kq+d8Xc5UJFwg2APVVORMxcDXpoJzE82Zio2ffSHnqSzOVFT8t7XnPKHMmYqH97MznJd1OFORkDjBtmEaHSU4UzGQPke6dDLdg3OmOs7tae5FUxocljPVWXJXKqZPbMqAnKkOUrjYNHF64Yc6Rfl6YeCPTgB/qLzkG/ILLIY/1F+1D7aMDOMPTTdehNnMRfKH1ntnmia/rVOcqbbRphDiJzbj+UPfHWzFRhzpFGeqDfQoRLYdBzvFmWo1nQpx05TjneJMtZR+hSi15lSnOFOtY0ghqhcOm+FMtYhRhahYOByBM9UKrFAeH0jl8XJGHm8q5PHWXh6fYJPHp7nl8cWm3IslaFDYMfkN/eJMtZNahd3TXtovzlSbqVI4OOFF/eJMtZ+ywilTnd4vzlRHKCicOMnwQ50ip/BDPjrqFm8VftQXmLTFtMIPXEbqWkwo/NjNnKjFd4XbptFUiDMVCf8p3DyBynKcqXj4UXgkerEoZyoqHv/ynQ1dWY4kFZvRC6f9VVbkTMXAddxfTV3OVCTkljOb4Txw8Vv8C7ZWHWkHRFKmAAAAAElFTkSuQmCC\n"},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["images = ttt.generate_image_history(history)\n","ttt.display_sequence(images)"],"metadata":{"id":"lgiYWJdX4YwL","colab":{"base_uri":"https://localhost:8080/","height":216,"referenced_widgets":["e031bea0898043ed930010d96b72f68f","bb1be023aa6c43699e50e67626f45871","61e51b580f0e42f9aeaf8ce87c068866","6ba4e6e248b74d86a19ffa34954a49f9","81eb6ff3d67c4c7fa9eb8a3aa7deaf45","7369d50ed4364508af84bfce1d0f666f","e6c5151702da464a8f43e8b0d040b450"]},"executionInfo":{"status":"ok","timestamp":1701278567392,"user_tz":180,"elapsed":18,"user":{"displayName":"Ulisses Martins Dias","userId":"04658399356803049089"}},"outputId":"cbc0be5a-a7c9-4051-f49c-2bb59f1051f2"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["interactive(children=(IntSlider(value=2, description='frame', max=5), Output()), _dom_classes=('widget-interac…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e031bea0898043ed930010d96b72f68f"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["<function tic_tac_toe.display_sequence.<locals>._show(frame=(0, 5))>"]},"metadata":{},"execution_count":25}]}]}